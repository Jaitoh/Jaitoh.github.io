<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="概括">
<meta property="og:type" content="article">
<meta property="og:title" content="[Reading in detail] Prefrontal Cortex and Decision Making in a Mixed Strategy Game">
<meta property="og:url" content="http://yoursite.com/2020/03/24/Reading-2003-Reading-Detailed-Prefrontal-Cortex-and-Decision-Making-in-a-Mixed-Strategy-Game/index.html">
<meta property="og:site_name" content="Jaitoh">
<meta property="og:description" content="概括">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-03-24T22:30:31.000Z">
<meta property="article:modified_time" content="2021-07-23T18:18:17.881Z">
<meta property="article:author" content="Jaitoh">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/03/24/Reading-2003-Reading-Detailed-Prefrontal-Cortex-and-Decision-Making-in-a-Mixed-Strategy-Game/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>[Reading in detail] Prefrontal Cortex and Decision Making in a Mixed Strategy Game | Jaitoh</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=<meta name="google-site-verification" content="q5bDFpvd5SIkNAE9RNamt1KMyHJIVh2x1znD0w9tiRA" /># <app_id>"></script>
    <script pjax>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', '<meta name="google-site-verification" content="q5bDFpvd5SIkNAE9RNamt1KMyHJIVh2x1znD0w9tiRA" /># <app_id>');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Jaitoh" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <link rel="stylesheet" href="/dist/APlayer.min.css">
  <div id="aplayer"></div>
  <script type="text/javascript" src="/dist/APlayer.min.js"></script>
  <script type="text/javascript" src="/dist/music.js"></script>

  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jaitoh</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-coding">

    <a href="/categories/coding" rel="section"><i class="fa fa-fw fa-heartbeat"></i>Coding</a>

  </li>
        <li class="menu-item menu-item-readings">

    <a href="/categories/Readings" rel="section"><i class="fa fa-fw fa-book"></i>Readings</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/jaitoh" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/24/Reading-2003-Reading-Detailed-Prefrontal-Cortex-and-Decision-Making-in-a-Mixed-Strategy-Game/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jaitoh">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jaitoh">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [Reading in detail] Prefrontal Cortex and Decision Making in a Mixed Strategy Game
        </h1>

        <div class="post-meta">

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-24 23:30:31" itemprop="dateCreated datePublished" datetime="2020-03-24T23:30:31+01:00">2020-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-23 20:18:17" itemprop="dateModified" datetime="2021-07-23T20:18:17+02:00">2021-07-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reading/" itemprop="url" rel="index"><span itemprop="name">Reading</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="firestore-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <center>

<p>概括</p>
</center>

<a id="more"></a>

<h1 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract$^2$:"></a>Abstract$^2$:</h1><blockquote>
<p>In a multi-agent environment, where the outcomes of one’s actions change dynamically because they are related to the behavior of other beings, it becomes difficult to make an optimal decision about how to act. Although game theory provides normative solutions for decision making in groups, how such decision-making strategies are altered by experience is poorly understood. These adaptive processes might resemble reinforcement learning algorithms, which provide a general framework for finding optimal strategies in a dynamic environment. Here we investigated the role of prefrontal cortex (PFC) in dynamic decision making in monkeys. As in reinforcement learning, the animal’s choice during a competitive game was biased by its choice and reward history, as well as by the strategies of its opponent. Furthermore, neurons in the dorsolateral prefrontal cortex (DLPFC) encoded the animal’s past decisions and payoffs, as well as the conjunction between the two, providing signals necessary to update the estimates of expected reward. Thus, PFC might have a key role in optimizing decision-making strategies.</p>
</blockquote>
<h1 id="Authors-and-institutes"><a href="#Authors-and-institutes" class="headerlink" title="Authors and institutes:"></a>Authors and institutes:</h1><p>Dominic J Barraclough,Michelle L Conroy &amp; Daeyeol Lee</p>
<h1 id="content"><a href="#content" class="headerlink" title="content:"></a>content:</h1><blockquote>
<p>Decision making refers to an evaluative process of selecting a particular action from a set of alternatives. When the mapping between a particular action and its outcome or utility is fixed, the decision to select the action with maximum utility can be considered optimal or rational. However, animals face more difficult problems in a multiagent environment, in which the outcome of one’s decision can be influenced by the decisions of other animals. Game theory provides a mathematical framework to analyze decision making in a group of agents1–4.A game is defined by a set ofactions available to each player, and a payoff matrix that specifies the reward or penalty for each player as a function of decisions made by all players. A solution or equilibrium in game theory refers to a set of strategies selected by a group of rational players1,5,6.Nash has proved that any n-player competitive game has at least one equilibrium in which no players can benefit by changing their strategies individually5.These equilibrium strategies often take the form ofa mixed strategy, which is defined as a probability density function over the alternative actions available to each player. This requires players to choose randomly among alternative choices, as in the game of rock-paper-scissors during which choosing one of the alternatives (e.g., paper) exclusively allows the opponent to exploit such a biased choice (with scissors).</p>
</blockquote>
<blockquote>
<p>Many studies have shown that people frequently deviate from the predictions of game theory7–21.Although the magnitudes of such deviations are often small, they have important implications regarding the validity of assumptions in game theory, such as the rationality of human decision-makers22–27.In addition, strategies of human decision-makers change with their experience17–21.These adaptive processes might be based on reinforcement learning algorithms28, which can be used to approximate optimal decision-making strategies in a dynamic environment. In the present study, we analyzed the performance of monkeys playing a zero-sum game against a computer opponent to determine how closely their behaviors match the predictions of game theory and whether reinforcement learning algorithms can account for any deviations from such predictions. In addition, neural activity was recorded from the DLPFC to investigate its role during strategic decision making in a multi-agent environment. The results showed that the animal’s choice behavior during a competitive game could be accounted for by a reinforcement learning algorithm. Individual prefrontal neurons often modulated their activity according to the choice of the animal in the previous trial, the outcome of that choice, and the conjunction between the choice and its outcome. This suggests that the PFC may be involved in updating the animal’s decision-making strategy based on a reinforcement learning algorithm.</p>
</blockquote>
<p>RESULTS<br>Behavioral performance</p>
<blockquote>
<p>Two rhesus monkeys played a game analogous to matching pennies against a computer in an oculomotor free-choice task (Fig. 1a; Methods). The animal was rewarded when it selected the same target as the computer that was programmed to minimize the animal’s reward by exploiting the statistical bias in the animal’s choice behavior. Accordingly, the optimal strategy for the animal was to choose the targets randomly with equal probabilities, which corresponds to the Nash equilibrium in the matching pennies game. To determine how the animal’s decisions were influenced by the strategy of the opponent, we manipulated the amount of information that was used by the computer opponent (see Methods). In algorithm 0, the computer selected its targets randomly with equal probabilities, regardless of the animal’s choice patterns. In algorithm 1, the computer analyzed only the animal’s choice history, but not its reward history. In algorithm 2, both choice and reward histories were analyzed. In both algorithms 1 and 2, the computer chose its target randomly if it did not find any systematic bias in the animal’s choice behavior. Therefore, a reward rate near 0.5 indicates that the animal’s performance was optimal.</p>
</blockquote>
<blockquote>
<p>Indeed, the animal’s reward rate was close to 0.5 for all algorithms, indicating that the animal’s performance was nearly optimal. In algorithm 0, the reward rate was fixed at 0.5 regardless of the animal’s behavior, and therefore there was no incentive for the animal to choose the targets with equal probabilities. In fact, both animals chose the right-hand target more frequently (P = 0.70 and 0.90 and n = 5,327 and 1,669 trials, for the two animals, respectively) than the left-hand target. For the remaining two algorithms, the probability ofchoosing the righthand target was much closer to 0.5 (Fig. 1c), which corresponds to the Nash equilibrium of the matching pennies game. In addition, the probability of choosing a given target was relatively unaffected by the animal’s choice in the previous trial. For example, the probability that the animal would select the same target as in the previous trial was also close to 0.5 (P = 0.51 ± 0.06 and 0.50 ± 0.04 and n = 120,254 and 74,113 trials, for algorithms 1 and 2, respectively).<br>In contrast, the animal’s choice was strongly influenced by the computer’s choice in the previous trial, especially in algorithm 1. In the game of matching pennies, the strategy to choose the same target selected by the opponent in the previous trial can be referred to as a win-stay-lose-switch (WSLS) strategy, as this is equivalent to choosing the same target as in the previous trial if that choice was rewarded and choosing the opposite target otherwise. The probability of the WSLS strategy in algorithm 1 (0.73 ± 0.14) was significantly higher than that in algorithm 2 (0.53 ± 0.06; P &lt;10−16; Fig. 1d). Although the tendency for the WSLS strategy in algorithm 2 was only slightly above chance, this bias was still statistically significant (P &lt;10−5). Similarly, average mutual information between the sequence of animal’s choice and reward in three successive trials and the animal’s choice in the following trial decreased from 0.245 (± 0.205) bits for algorithm 1 to 0.043 (± 0.035) bits for algorithm 2.</p>
</blockquote>
<p>Reinforcement learning model</p>
<blockquote>
<p>Using a reinforcement learning model19–21,28,29,we tested whether the animal’s decision was systematically influenced by the cumulative effects of reward history. In this model, a decision was based on the difference between the value functions (that is, expected reward) for the two targets. Denoting the value functions ofthe two targets (L and R) at trial t as Vt(L) and Vt(R), the probability ofchoosing each target is given by the logit transformation of the difference between the value functions30.In other words,</p>
</blockquote>
<blockquote>
<p>logit P(R) ≡ log P(R)/(1 − P(R)) = Vt(R) − Vt(L).</p>
</blockquote>
<blockquote>
<p>The value function, Vt(x), for target x, was updated after each trial according to the following:<br>Vt+1(x) = αVt(x) + ∆t(x),<br>where α is a discount factor, and ∆t(x) denotes the change in the value function determined by the animal’s decision and its outcome. In the current model, ∆t(x) = ∆1 if the animal selects the target x and is rewarded, ∆t(x) = ∆2 if the animal selects the target x and is not rewarded, and ∆t(x) = 0 if the animal does not select the target x. We introduced a separate parameter for the unrewarded target (∆2) because the probability ofchoosing the same target after losing a reward was significantly different from the probability of switching to the other target for all animals and for both algorithms 1 and 2. Maximum likelihood estimates31 of the model parameters (Table 1) showed that a frequent use of the WSLS strategy during algorithm 1 was reflected in a relatively small discount factor (α &lt; 0.2), a large positive ∆1 (&gt; 0.6) and a large negative ∆2 (&lt; −0.5) in both animals. For algorithm 1, this led to a largely bimodal distribution for the difference in the value functions (Fig. 1e). In contrast, the magnitude of changes in value function during algorithm 2 was smaller, indicating that the outcome of previous choices only weakly influenced the subsequent choice of the animal. In addition, the discount factor for algorithm 2 was relatively large (α &gt; 0.8). This suggests that the animal’s choice was systematically influenced by the combined effects of previous reward history even in algorithm 2. The combination of model parameters for algorithm 2 produced an approximately normal distribution for the difference in value functions (Fig. 1e). This implies that for most trials, the difference in the value functions of the two targets was relatively small, making it difficult to predict the animal’s choice reliably. These results suggest that during a competitive game, the monkeys might have approximated the optimal decision-making strategy using a reinforcement learning algorithm.</p>
</blockquote>
<p>Prefrontal activity during a competitive game</p>
<blockquote>
<p>The value functions in the above reinforcement learning model were updated according to the animal’s decisions and the outcomes of those decisions. To determine whether such signals are encoded in the activity of individual neurons in PFC, we recorded single-neuron activity in the DLPFC while the animal played the same free-choice task. During the neurophysiological recording, the computer selected its target according to algorithm 2. A total of 132 neurons were examined during a minimum of 128 free-choice trials (mean = 583 trials; Fig. 1b). As a control, each neuron was also examined during 128 trials of a visual search task in which the animal’s decision was guided by sensory stimuli (Methods).</p>
</blockquote>
<blockquote>
<p>During the free-choice trials, activity in some prefrontal neurons was influenced by the difference in the value functions for the two targets (that is, V(R) − V(L)), although the effects in individual neurons were relatively small (Fig. 2). This was not entirely due to the animal’s choice and its outcome in the previous trial, as the value functions estimated for the previous trial produced similar results (Fig. 2a). If individual PFC neurons are involved in the temporal integration of value functions according to the reinforcement learning model described above, differences in the value functions (i.e., V(x)) and their changes (i.e., ∆(x)) would similarly influence the activity ofPFC neurons. Interestingly, such patterns were found for the delay and movement periods, but not for the fore-period (Methods; Fig. 2b). These results suggest that some prefrontal neurons might be involved in temporally integrating the signals related to previous choice and its outcome to update value functions.</p>
</blockquote>
<blockquote>
<p>To examine how the activity of individual PFC neurons is influenced by the animal’s choices and their outcomes, we analyzed neural activity by three-way ANOVA with the animal’s choice and reward in the previous trial and its choice in the current trial as main factors. For 39% of PFC neurons, the activity during the fore-period was influenced by the animal’s choice in the previous trial (Fig. 3). For example, in the neuron illustrated in Figure 4, the animal’s choice in the previous trial exerted a significant influence on the activity before and during the fore-period, as well as during the delay period (3-way ANOVA, P &lt; 0.001). In addition, activity during the movement period was still influenced by the animal’s choice in the previous trial and its outcome, as well as by their interactions with the animal’s choice in the current trial. To determine whether any ofthese effects could be attributed to systematic variability in eye movements, the above analysis was repeated using the residuals from a regression model in which the neural activity related to a set of eye movement parameters was factored out (Methods). The results were nearly identical, with the only difference found in the loss of significance for the effect of the current choice. During the fore-period, 35% of neurons showed a significant effect of the animal’s choice in the previous trial on the residuals from the same regression model.</p>
</blockquote>
<blockquote>
<p>It is possible that the animal’s choice in the previous trial influenced the activity of this neuron during the next trial through systematic changes in unidentified sensorimotor events, such as licking or eye movements during the inter-trial interval, that were not experimentally controlled. This was tested by comparing the activity of the same neuron in the search and free-choice trials. For the neuron shown in Figure 4,activity during search trials was significantly affected by the position of the target in the previous trial only during the fore-period, and this effect was opposite to and significantly different from that found in the free-choice trials (4-way ANOVA, P &lt;10−5). The raster plots show that this change occurred within a few trials after the animal switched from search to free-choice trials (Fig. 4). These results suggest that the effect of the animal’s choice in the previous trial on the activity of this neuron did not merely reflect nonspecific sensorimotor events. In 17% ofthe neurons that showed a significant effect of the animal’s previous choice during the fore-period, there was also a significant interaction between the task type (search vs. free-choice) and the animal’s choice in the previous trial (Fig. 3). This indicates that signals related to the animal’s past choice were actively maintained in the PFC according to the type of decision. It is unlikely that this was entirely due to an ongoing drift in the background activity (i.e., non-stationarity), as the control analysis performed on two successive blocks of free-choice trials did not produce a single case with the same effect during the fore-period (Fig. 3).</p>
</blockquote>
<blockquote>
<p>During the fore-period, 39% of neurons showed a significant effect ofthe reward in the previous trial. For example, the activity ofthe neuron in Figure 5 was higher throughout the entire trial after the animal was not rewarded in the previous trial, compared to when the animal was rewarded. This effect was nearly unchanged when we removed the eye-movement related activity in a regression analysis, both in this single-neuron example and for the population as a whole. Overall, 37% of neurons showed the effect of the previous reward when the analysis was performed on the residuals from the regression model. The possibility that this effect was entirely due to uncontrolled sensorimotor events is also unlikely, as a substantial proportion of these neurons (21%) also showed a significant interaction between the task type and the previous reward during the fore-period (Fig. 3). To update the value functions in a reinforcement learning model, signals related to the animal’s choice and its outcome must be combined, because each variable alone does not specify how the value function ofa particular target should be changed. Similarly, activity of the neurons in the PFC was often influenced by the conjunction of these two variables. In the neuron in Figure 6, for example, there was a gradual buildup of activity during the fore-period, but this occurred only when the animal had selected the right-hand target in the previous trial, and this choice was not rewarded. During the delay period, the activity ofthis neuron diverged to reflect the animal’s choice in the current trial (Fig. 6,arrow). The same neuron showed markedly weaker activity during the search trials, suggesting that information coded in the activity ofthis neuron regarding the outcome ofchoosing a particular target was actively maintained in free-choice trials (Fig. 6).</p>
</blockquote>
<blockquote>
<p>For the fore-period, 20% of the neurons showed significant interaction between the animal’s choice and its outcome in the previous trial (P &lt; 0.05; Fig. 3).Activity related to eye movements was not an important factor: 90% of these neurons showed the same effect in the residuals from the regression analysis that factored out the effects of eye movements. Furthermore, during the fore-period, 27% of the same neurons showed significant threeway interactions among task type, animal’s choice in the previous trial and outcome of the previous trial. In contrast, the control analysis during the first two blocks ofthe freechoice task revealed such an effect only in 5% of the neurons (Fig. 3). These results indicate that signals related to the conjunction of the animal’s previous decision and its outcome are processed differently in the PFC according to the type ofdecisions made by the animal.</p>
</blockquote>
<p>DISCUSSION</p>
<blockquote>
<p>Interaction with other intelligent beings is fundamentally different from—and more complex than—dealing with inanimate objects32,33.Interactions with other animals are complicated by the fact that their behavioral strategies often change as a result of one’s own behavior. Therefore, the analysis of decision making in a group requires a more sophisticated analytical framework, which is provided by game theory. Matching pennies is a relatively simple zero-sum game that involves two players and two alternative choices. The present study examined the behavior of monkeys playing a competitive game similar to matching pennies against a computer opponent. It is not known whether monkeys treated this game as a competitive situation with another intentional being. Nevertheless, the same formal framework of game theory is applicable to the task used in this study, and as predicted, the animal’s behavior was influenced by the opponent’s strategy. When the computer blindly played the equilibrium strategy regardless of the animal’s behavior, the animals selected one of the targets more frequently. In contrast, when the computer opponent began exploiting biases in the animal’s choice sequence, the animal’s behavior approached the equilibrium strategy. Furthermore, when the computer did not examine the animal’s reward history (algorithm 1), the animals achieved a nearly optimal reward rate by adopting the win-stay-lose-switch (WSLS) strategy. This was possible because this strategy was not detected by the computer. Finally, the frequency of the WSLS strategy was reduced when the computer began exploiting biases in the animal’s choice and reward sequences (algorithm 2).</p>
</blockquote>
<blockquote>
<p>These results also suggest that the animals approximated the optimal strategy using a reinforcement learning algorithm. This model assumes that the animals base their decisions, in part, on the estimates of expected rewards for the two targets and tend to select the target with larger expected reward. During zero-sum games such as matching pennies, strategies ofthe players behaving according to some reinforcement learning algorithms would gradually converge on a set of equilibrium strategies5–7.However, it is important to update the value functions of different targets by a small amount after each play when playing against a fully informed rational player (such as algorithm 2 in the present study). This is because large, predictable changes in the value functions would reveal one’s next choice to the opponent. In the present study, the magnitude of changes in the value function varied according to the strategy of the opponent and was adjusted through the animal’s experience.</p>
</blockquote>
<blockquote>
<p>Finally, neurophysiological recordings in the PFC revealed a potential neural basis for updating the value functions adaptively while interacting with a rational opponent. Reward-related activity is widespread in the brain34–38.In particular, signals related to expected reward (i.e.,value functions) are present in various brain areas39–43, including the DLPFC44–48. Our results showed that neurons in the DLPFC also code signals related to the animal’s choice in the previous trial. Such signals might be actively maintained and processed differently in the DLPFC according to the type of information required for the animal’s upcoming decisions. Furthermore, signals related to the animal’s past choices and their outcomes are combined at the level of individual PFC neurons. These signals might then be temporally integrated according to a reinforcement learning algorithm to update the value functions for alternative actions. Many neurons in the PFC show persistent activity during a working memory task, and the same circuitry might be ideally suited for temporal integration of signals related to the animal’s previous choice and its outcome49.Although the present study examined the animal’s choice behavior in a competitive game, reinforcement learning algorithms can converge on optimal solutions for a wide range ofdecision-making problems in dynamic environments. Therefore, the results from the present study suggest that the PFC has an important role in optimizing decision-making strategies in a dynamic environment that may include multiple agents.</p>
</blockquote>
<p>METHODS</p>
<blockquote>
<p>Animal preparations. Two male rhesus monkeys were used. Their eye movements were monitored at a sampling rate of 250 Hz with either a scleral eye coil or a high-speed video-based eye tracker (ET49, Thomas Recording). All the procedures used in this study conformed to National Institutes of Health guidelines and were approved by the University of Rochester Committee on Animal Research.<br>Behavioral task.Monkeys were trained to play a competitive game analogous to matching pennies against a computer in an oculomotor free-choice task (Fig. 1a). During a 0.5-s fore-period, they fixated a small yellow square (0.9 × 0.9°; CIE x = 0.432, y = 0.494, Y = 62.9 cd/m2) in the center of a computer screen, and then two identical green disks (radius = 0.6°; CIE x = 0.286, y = 0.606, Y = 43.2 cd/m2) were presented 5° away in diametrically opposed locations. The central target disappeared after a 0.5-s delay period, and the animal was required to shift its gaze to one ofthe targets.At the end ofa 0.5-s hold period, a red ring (radius = 1°; CIE x = 0.632, y = 0.341, Y = 17.6 cd/m2) appeared around the target selected by the computer, and the animal maintained its fixation for another 0.2 s. The animal was rewarded at the end of this second hold period, but only if it selected the same target as the computer. The computer had been programmed to exploit certain biases displayed by the animal in making its choices. Each neuron was also tested in a visual search task. This task was identical to the free-choice task, except that one of the targets in the free-choice task was replaced by a distractor (red disk). The animal was required to shift its gaze toward the remaining target (green disk), and this was rewarded randomly with 50% probability. This made it possible to examine the effect of reward on the neural activity. The location of the target was selected from the two alternative locations pseudo-randomly for each search trial.</p>
</blockquote>
<blockquote>
<p>Algorithms for computer opponent. During the free-choice task, the computer selected its target according to one ofthree different algorithms. In algorithm 0, the computer selected the two targets randomly with equal probabilities, which corresponds to the Nash equilibrium in the matching pennies game. In algorithm 1, the computer exploited any systematic bias in the animal’s choice sequence to minimize the animal’s reward rate. The computer saved the entire history of the animal’s choices in a given session, and used this information to predict the animal’s next choice by testing a set of hypotheses. First, the conditional probabilities of choosing each target given the animal’s choices in the preceding n trials (n =0 to 4) were estimated. Next, each of these conditional probabilities was tested against the hypothesis that the animal had chosen both targets with equal probabilities. When none of these hypotheses was rejected, the computer selected each target randomly with 50% probability, as in algorithm 0. Otherwise, the computer biased its selection according to the probability with the largest deviation from 0.5 that was statistically significant (binomial test, P &lt; 0.05). For example, if the animal chose the right-hand target with 80% probability, the computer selected the left-hand target with the same probability. Therefore, to maximize reward, animals needed to choose both targets with equal frequency and select a target on each trial independently from previous choices. In algorithm 2, the computer exploited any systematic bias in the animal’s choice and reward sequences. In addition to the hypotheses tested in algorithm 1, algorithm 2 also tested the hypothesis that the animal’s decisions were independent ofprior choices and their payoffs in the preceding n trials (n =1 to 4). Thus, to maximize total reward in algorithm 2, it was necessary for the animal to choose both targets with equal frequency and to make choices independently from previous choices and payoffs.</p>
</blockquote>
<blockquote>
<p>Neurophysiological recording. Single-unit activity was recorded from the neurons in the DLPFC of two monkeys using a five-channel multi-electrode recording system (Thomas Recording). The placement ofthe recording chamber was guided by magnetic resonance images, and this was confirmed in one animal by metal pins inserted in known anatomical locations. In addition, the frontal eye field (FEF) was defined in both animals as the area in which saccades were evoked by electrical stimulations with currents &lt;50 µA (ref. 50). All the neurons described in the present study were anterior to the FEF.</p>
</blockquote>
<blockquote>
<p>Analysis ofbehavioral data. The frequency ofa behavioral event (e.g.,reward) was examined with the corresponding probability averaged across recording sessions and its standard deviation. The values of mutual information were corrected for the finite sample size. Null hypotheses in the analysis of behavioral data were tested using a binomial test or a t-test (P &lt; 0.05). Parameters in the reinforcement learning model were estimated by a maximum likelihood procedure, using a function minimization algorithm in Matlab (Mathworks Inc.), and confidence intervals were estimated by profile likelihood intervals31.</p>
</blockquote>
<blockquote>
<p>Analysis of neural data. Spikes during a series of 500-ms bins were counted separately for each trial. The effects of the animal’s choice (P) and reward (R) in the previous trial and the choice in the current trial (C) were analyzed in a 3-way ANOVA (P × R × C). The effect of the task (search versus free-choice) was analyzed in a four-way ANOVA (Task × P × R × C).As a control analysis to determine whether the task effect was due to non-stationarity in neural activity, the same four-way ANOVA was performed for the first two successive blocks of 128 trials in the free-choice task (Fig. 3). To determine whether eye movements were confounding factors, the above analysis was repeated using the residuals from the following regression model:</p>
</blockquote>
<blockquote>
<p>S= a0 + a1 Xpre80 + a2 Ypre80 + a3 XFP + a4 YFP + a5 XSV + a6 YSV + a7 SRT + a8 PV + ε</p>
</blockquote>
<blockquote>
<p>where S indicates the spike count, Xpre80 (Ypre80) the horizontal (vertical) eye position 80 ms before the onset ofcentral fixation target, XFP (YFP) the average horizontal (vertical) eye position during the fore-period, XSV (YSV) the horizontal (vertical) component of the saccade directed to the target, SRT and PV the saccadic reaction time and the peak velocity of the saccade, and ε the error term.</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/24/Coding-2003-Markdown-Cheatsheet/" rel="prev" title="[SUMMARY] cheatsheets">
      <i class="fa fa-chevron-left"></i> [SUMMARY] cheatsheets
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/25/Coding-2003-python-for-in%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/" rel="next" title="[Coding] python for in 高级用法">
      [Coding] python for in 高级用法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract-2"><span class="nav-number">1.</span> <span class="nav-text">Abstract$^2$:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Authors-and-institutes"><span class="nav-number">2.</span> <span class="nav-text">Authors and institutes:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#content"><span class="nav-number">3.</span> <span class="nav-text">content:</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jaitoh</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jaitoh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jaitoh" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:He_Wenjie@outlook.com" title="E-Mail → mailto:He_Wenjie@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jaitoh</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        




  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-app.js"></script>
  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-firestore.js"></script>
  <script>
    firebase.initializeApp({
      apiKey   : 'AIzaSyDdQ6s_Kqq1eGNG5fabYXQhhSxgVLVEyMs',
      projectId: 'jaitohweb'
    });

    function getCount(doc, increaseCount) {
      // IncreaseCount will be false when not in article page
      return doc.get().then(d => {
        var count = 0;
        if (!d.exists) { // Has no data, initialize count
          if (increaseCount) {
            doc.set({
              count: 1
            });
            count = 1;
          }
        } else { // Has data
          count = d.data().count;
          if (increaseCount) {
            // If first view this article
            doc.set({ // Increase count
              count: count + 1
            });
            count++;
          }
        }

        return count;
      });
    }

    function appendCountTo(el) {
      return count => {
        el.innerText = count;
      }
    }
  </script>
  <script pjax>
    (function() {
      var db = firebase.firestore();
      var articles = db.collection('articles');

      if (CONFIG.page.isPost) { // Is article page
        var title = document.querySelector('.post-title').innerText.trim();
        var doc = articles.doc(title);
        var increaseCount = CONFIG.hostname === location.hostname;
        if (localStorage.getItem(title)) {
          increaseCount = false;
        } else {
          // Mark as visited
          localStorage.setItem(title, true);
        }
        getCount(doc, increaseCount).then(appendCountTo(document.querySelector('.firestore-visitors-count')));
      } else if (CONFIG.page.isHome) { // Is index page
        var promises = [...document.querySelectorAll('.post-title')].map(element => {
          var title = element.innerText.trim();
          var doc = articles.doc(title);
          return getCount(doc);
        });
        Promise.all(promises).then(counts => {
          var metas = document.querySelectorAll('.firestore-visitors-count');
          counts.forEach((val, idx) => {
            appendCountTo(metas[idx])(val);
          });
        });
      }
    })();
  </script>




      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      script.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://yoursite.com/2020/03/24/Reading-2003-Reading-Detailed-Prefrontal-Cortex-and-Decision-Making-in-a-Mixed-Strategy-Game/',]
      });
      });
  </script>

    </div>  
</body>
</html>
